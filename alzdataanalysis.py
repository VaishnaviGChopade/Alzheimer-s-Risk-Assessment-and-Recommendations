# -*- coding: utf-8 -*-
"""AlzDataanalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R-jYkkDscgU5T6EvAfNEUySxHz5cXq0b
"""

import pandas as pd
df=pd.read_csv("/content/alzheimers_disease_data.csv")
df
df.drop('DoctorInCharge', axis=1, inplace=True)

df

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(50, 20))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.savefig('correlation_matrix.png')
plt.show()

correlation_df = pd.DataFrame(correlation_matrix)

correlation_df.to_csv('correlation_matrix.csv')

target_correlation = correlation_matrix['Diagnosis'].sort_values(ascending=False)  # Replace 'target' with your column name
print("Correlation with Target Variable:")
print(target_correlation)

X = df.drop('Diagnosis', axis=1)
y = df['Diagnosis']

X.shape

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=32)

rf = RandomForestClassifier(class_weight='balanced', random_state=42)
rf = rf.fit(X_train, y_train)

feature_importances = rf.feature_importances_


importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

importance_df = importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 20))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()
print(importance_df)
importance_df.to_csv('fea_imp_rf.csv')

df['Diagnosis'].value_counts()

"""PC1 explains ~3.7% of the total variance

PC2 explains ~3.6%

Combined: Only ~7.3% of your data’s total information is captured by these 2 components

💡 That’s very low! Usually, you'd want at least 70–90% total variance explained when using PCA for dimensionality reduction.
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Drop the target column for features
X = df.drop('Diagnosis', axis=1)
y = df['Diagnosis']

# Train a temp Random Forest to get feature importances
rf_temp = RandomForestClassifier(class_weight='balanced', random_state=42)
rf_temp.fit(X, y)

# Get feature importances
importances = rf_temp.feature_importances_
feature_names = X.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Filter features (importance >= 0.01) and exclude 'PatientID'
selected_features = importance_df[
    (importance_df['Importance'] >= 0.01) &
    (importance_df['Feature'] != 'PatientID')
]['Feature'].tolist()

# Reduce the dataset
X_filtered = df[selected_features]

# Print number and names of selected features
print(f"\n✅ Number of selected features: {len(selected_features)}")
print("✅ Selected Features:", selected_features)

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.2, random_state=32)

# Train the final model with regularization
rf_model = RandomForestClassifier(
    n_estimators=200,             # number of trees
    max_depth=10,                 # limit depth of each tree
    min_samples_split=10,         # min samples required to split a node
    min_samples_leaf=5,           # min samples at leaf

    class_weight='balanced',      # handle class imbalance
    random_state=42
)
rf_model.fit(X_train, y_train)

# Evaluate on training data
y_train_pred = rf_model.predict(X_train)
print("\n=== Training Results ===")
print("Confusion Matrix:\n", confusion_matrix(y_train, y_train_pred))
print("\nClassification Report:\n", classification_report(y_train, y_train_pred))
print("Accuracy:", accuracy_score(y_train, y_train_pred))

# Evaluate on testing data
y_test_pred = rf_model.predict(X_test)
print("\n=== Testing Results ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))
print("Accuracy:", accuracy_score(y_test, y_test_pred))

import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier

# Assuming 'df' is your DataFrame with the target 'Diagnosis'

# --- Step 1: Feature Selection ---
# Separate features and target
X = df.drop('Diagnosis', axis=1)
y = df['Diagnosis']

# Use a temporary Random Forest model to estimate feature importances
rf_temp = RandomForestClassifier(class_weight='balanced', random_state=42)
rf_temp.fit(X, y)

# Create a DataFrame with feature importances
importances = rf_temp.feature_importances_
feature_names = X.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Filter features: keep those with Importance >= 0.01 and remove 'PatientID'
selected_features = importance_df[
    (importance_df['Importance'] >= 0.01) &
    (importance_df['Feature'] != 'PatientID')
]['Feature'].tolist()

# Reduce the dataset to only selected features
X_filtered = df[selected_features]

# Print number and names of selected features
print(f"\n✅ Number of selected features: {len(selected_features)}")
print("✅ Selected Features:", selected_features)

# --- Step 2: Train-Test Split ---
X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.2, random_state=32, stratify=y)

# --- Step 3: Train the XGBoost Model ---
xgb_model = XGBClassifier(
    # use_label_encoder=False,
    # eval_metric='logloss',
    random_state=42,
    reg_alpha=1.0,
    reg_lambda=1.0,
    # max_depth=3,
    n_estimators=100,
    learning_rate=0.01,
    scale_pos_weight= (len(y) - sum(y)) / sum(y)  # balances positive vs negative classes
)


xgb_model.fit(X_train, y_train)
# --- Step 4: Evaluate on Training Data ---
y_train_pred = xgb_model.predict(X_train)
print("\n=== Training Results (XGBoost) ===")
print("Confusion Matrix:\n", confusion_matrix(y_train, y_train_pred))
print("\nClassification Report:\n", classification_report(y_train, y_train_pred))
print("Accuracy:", accuracy_score(y_train, y_train_pred))

# --- Step 5: Evaluate on Testing Data ---
y_test_pred = xgb_model.predict(X_test)
print("\n=== Testing Results (XGBoost) ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))
print("\nClassification Report:\n", classification_report(y_test, y_test_pred))
print("Accuracy:", accuracy_score(y_test, y_test_pred))

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Step 1: Create new model (continuing training)
new_xgb_model = XGBClassifier(
     random_state=42,
    reg_alpha=1.0,
    reg_lambda=1.0,
    # max_depth=3,
    n_estimators=250,
    learning_rate=0.01,
    scale_pos_weight= (len(y) - sum(y)) / sum(y)
)

# Step 2: Continue training using base model
new_xgb_model.fit(X_train, y_train, xgb_model=xgb_model)

# Step 3: Evaluate on Training Data
y_train_pred_new = new_xgb_model.predict(X_train)
print("\n=== Training Results (Continued XGBoost) ===")
print("Confusion Matrix:\n", confusion_matrix(y_train, y_train_pred_new))
print("\nClassification Report:\n", classification_report(y_train, y_train_pred_new))
print("Accuracy:", accuracy_score(y_train, y_train_pred_new))

# Step 4: Evaluate on Testing Data
y_test_pred_new = new_xgb_model.predict(X_test)
print("\n=== Testing Results (Continued XGBoost) ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred_new))
print("\nClassification Report:\n", classification_report(y_test, y_test_pred_new))
print("Accuracy:", accuracy_score(y_test, y_test_pred_new))

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Step 1: Create new model (continuing training)
new_xgb_model1 = XGBClassifier(
     random_state=42,
    reg_alpha=1.0,
    reg_lambda=1.0,
    # max_depth=3,
    n_estimators=300,
    learning_rate=0.01,
    scale_pos_weight= (len(y) - sum(y)) / sum(y)
)

# Step 2: Continue training using base model
new_xgb_model1.fit(X_train, y_train, xgb_model=new_xgb_model)

# Step 3: Evaluate on Training Data
y_train_pred_new = new_xgb_model1.predict(X_train)
print("\n=== Training Results (Continued XGBoost1) ===")
print("Confusion Matrix:\n", confusion_matrix(y_train, y_train_pred_new))
print("\nClassification Report:\n", classification_report(y_train, y_train_pred_new))
print("Accuracy:", accuracy_score(y_train, y_train_pred_new))

# Step 4: Evaluate on Testing Data
y_test_pred_new = new_xgb_model.predict(X_test)
print("\n=== Testing Results (Continued XGBoost1) ===")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred_new))
print("\nClassification Report:\n", classification_report(y_test, y_test_pred_new))
print("Accuracy:", accuracy_score(y_test, y_test_pred_new))

import joblib

# Save the continued training model to a pickle file
joblib.dump(new_xgb_model1, 'continued_xgb_model1.pkl')

print("\nModel saved successfully as 'continued_xgb_model1.pkl'")

import joblib
import pandas as pd

# Load the saved model
model = joblib.load('continued_xgb_model1.pkl')

# Input data
new_data = [67, 23.78, 1, 2, 2.8, 2.8, 200, 72, 242.34, 7833.34, 162.98, 8, 12, 6, 1, 0, 8.78]

# Feature names (ensure correct order)
selected_features = [
    "Age", "BMI", "AlcoholConsumption", "PhysicalActivity", "DietQuality",
    "SleepQuality", "SystolicBP", "DiastolicBP", "CholesterolTotal",
    "CholesterolLDL", "CholesterolHDL", "CholesterolTriglycerides",
    "MMSE", "FunctionalAssessment", "MemoryComplaints",
    "BehavioralProblems", "ADL"
]

# Create DataFrame
new_input_df = pd.DataFrame([new_data], columns=selected_features)

# Predict
new_pred = model.predict(new_input_df)
new_prob = model.predict_proba(new_input_df)

# Get probability for class 1 (High Risk)
prob_high_risk = new_prob[0][1]

# Define thresholds
if prob_high_risk < 0.33:
    risk_level = "Low Risk"
elif prob_high_risk < 0.66:
    risk_level = "Moderate Risk"
else:
    risk_level = "High Risk"

# Output
print("\n🧠 Prediction for New Input:")
print(f"Predicted Class: {new_pred[0]}  (0 = Low Risk, 1 = High Risk)")
print(f"Probability of Risk: {prob_high_risk:.4f}")
print(f"🔍 Risk Level: {risk_level}")

